{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514b3999",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (Temp/ipykernel_1324/2047569568.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\froke\\AppData\\Local\\Temp/ipykernel_1324/2047569568.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    'C:\\Users\\froke\\jupyter_notebook_files\\anomaly_detection\\anomaly-detection\\disagreement_matrix\\engineering_version\\')\u001b[0m\n\u001b[1;37m                                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, \n",
    "                'C:\\Users\\froke\\jupyter_notebook_files\\anomaly_detection\\anomaly-detection\\disagreement_matrix\\engineering_version\\')\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from utils.data_loader import DataLoader\n",
    "from utils.build_classifiers import Classifiers\n",
    "import multiprocessing as mp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from IPython.display import display\n",
    "from utils.calculate_disagreement import Disagreement\n",
    "from utils.rank_score_diff import RankScoreDiff\n",
    "from utils.threshold_method import *\n",
    "from utils.construct_method import Constructor\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa407b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main idea: 一个外套走完所有数据集\n",
    "# read data\n",
    "def load_data():\n",
    "    dataloader = DataLoader()\n",
    "    train_X, train_y = dataloader.get_train_data()\n",
    "    return train_X, train_y\n",
    "\n",
    "\n",
    "def filter_big_data(train_X, train_y, MB):\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    for fname, data in train_X.items():\n",
    "        df_tmp = pd.DataFrame(data)\n",
    "        a.append(fname)\n",
    "        b.append(round(df_tmp.memory_usage(deep=True).sum() * 1e-6, 4))\n",
    "        c.append(str(round(sum(train_y[fname]) / len(train_y[fname]) * 100, 4)) + \"%\")\n",
    "    df_discription = pd.DataFrame({\"FileName\": a, \"Memory_in_MB\": b, \"OutliersRate\": c})\n",
    "    small_sets = set(df_discription.FileName[df_discription.Memory_in_MB <= MB])\n",
    "    new_train_X = dict()\n",
    "    new_train_y = dict()\n",
    "    for fname, data in train_X.items():\n",
    "        if fname in small_sets:\n",
    "            new_train_X[fname] = train_X[fname]\n",
    "            new_train_y[fname] = train_y[fname]\n",
    "    return new_train_X, new_train_y\n",
    "\n",
    "\n",
    "def train_classifiers():\n",
    "    x, y = load_data()\n",
    "    training_results = dict()\n",
    "    train_X, train_y = filter_big_data(x, y, 60)\n",
    "    normalizer = RobustScaler # RobustScaler\n",
    "    score_dfs = dict()\n",
    "    clf_initializer = Classifiers('mixed')\n",
    "    main_detectors = dict()\n",
    "    performances = dict()\n",
    "    mp.freeze_support()\n",
    "    with mp.Pool(10) as pool:\n",
    "        # files, data\n",
    "        # data is an numpy ndarray\n",
    "        for fname, data in train_X.items():\n",
    "            # base classifiers\n",
    "            training_results[fname] = pool.apply_async(clf_initializer.train_helper, (data, train_y[fname], normalizer, fname))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for key in training_results:\n",
    "            training_results[key] = training_results[key].get(timeout=5)\n",
    "    return training_results\n",
    "\n",
    "\n",
    "def get_disagreement(score_df, rsdiff_obj):\n",
    "    score_ndarray = score_df.to_numpy()\n",
    "    dis_obj = Disagreement(score_ndarray)\n",
    "    disagreement_df = dis_obj.calc_disagreement(rsdiff_obj.get_rsd_matrix_2d())\n",
    "    return disagreement_df\n",
    "\n",
    "\n",
    "def get_rsd_obj(score_nd_array):\n",
    "    tmp_var = RankScoreDiff(score_nd_array)\n",
    "    rsdiff_obj = tmp_var.generate_rank_score_diff_property()\n",
    "    return rsdiff_obj\n",
    "\n",
    "\n",
    "def get_thresholders(disagreement_nd_array,zeros_like=False):\n",
    "    s = np.apply_along_axis(get_thresholder_results, 0, disagreement_nd_array, zeros_like)\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_construct_methods(score_df, main_detector_name, rsd_obj, disagreement_result_ndarray, flatten_columns):\n",
    "    constructor = Constructor(score_df, main_detector_name, rsd_obj, disagreement_result_ndarray)\n",
    "    constructor.predict(flatten_columns)\n",
    "    return constructor.result_flatten_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rank_score_diff_obj_collection_dict = dict()\n",
    "    disagreement_collection_dict = dict()\n",
    "    thresholder_is_exceed_matrix_collection = dict()\n",
    "\n",
    "    ############################################################################################\n",
    "    '''\n",
    "    Step 1. Calculate Scores from Base Detectors for each file.\n",
    "    base_results is a dictionary with filename as key, and its value is a classifier object\n",
    "    which has 3 important components: \n",
    "        1. score_df: dataframe\n",
    "        2. performance: AUC DataFrame\n",
    "        3. String, Main detector's name\n",
    "    '''\n",
    "    # base results is a dictionary\n",
    "    # its key is file name and its value is a DataFrame with results from scores.\n",
    "    base_results = train_classifiers()\n",
    "    flatten_predict_score_df = dict()\n",
    "    ensemble_sequence = ['max', 'min', 'mean', 'median', 'sido', 'sodi','sum_self','sum_self_inverse']\n",
    "    disagreement_sequence = ['std', 'mad', 'sum_rsd', 'std_rsd', 'max_rsd']\n",
    "    threshold_sequence = ['2std', 'iqr', 'mad', 'std']\n",
    "    a_col = []\n",
    "    for en in ensemble_sequence:\n",
    "        for thr in threshold_sequence:\n",
    "            for dis in disagreement_sequence:\n",
    "                a_col.append(\"ensem_\" + en + \":\" + \"thr_\" + thr + \":\" + \"dis_\" + dis)\n",
    "    print(\"Base Classifier Trainning Completed, head to step2\")\n",
    "    ############################################################################################\n",
    "    '''\n",
    "    Step2. Calculate properties for each file \n",
    "    -   Disagreement matrix for each file. Disagreement has 5 calculation methods.\n",
    "        So, for each file, it is a DataFrame with Nx5 shape;\n",
    "    -   RSD_OBJ. Contains rank score difference object;\n",
    "    -   Thresholder. Contains 4 thresholder.\n",
    "    '''\n",
    "    auc_res = dict()\n",
    "    main_detector_dict = dict()\n",
    "    for fname in base_results.keys():\n",
    "        print(\"Processing \", fname)\n",
    "        # dataframe\n",
    "        score_df = base_results[fname].score_df\n",
    "        main_detector = base_results[fname].main_detector\n",
    "        main_detector_dict[fname] = main_detector\n",
    "        best_auc = base_results[fname].performance[main_detector]\n",
    "        rsd_obj = get_rsd_obj(score_df.to_numpy())\n",
    "        rank_score_diff_obj_collection_dict[fname] = rsd_obj\n",
    "        print(\"\\tDone in Calculate RSD Matrix\")\n",
    "\n",
    "\n",
    "        # dataframe\n",
    "        disagreement_collection_dict[fname] = get_disagreement(score_df, rsd_obj)\n",
    "        print(\"\\tDone in calculate disagreement\")\n",
    "\n",
    "        # matrix: [2std, iqr, mad, std]\n",
    "        thresholder_is_exceed_matrix_collection[fname] = get_thresholders(disagreement_collection_dict[fname].to_numpy(), zeros_like=False)\n",
    "        # write out\n",
    "        pd.DataFrame(thresholder_is_exceed_matrix_collection[fname][3, :, 4]).to_excel(\"/Users/kadima/experiment_any/anomaly-detection/disagreement_matrix/engineering_version/file_disagreement_records/\"+fname+\"_threshold.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\tDone in calculate thresholding\")\n",
    "\n",
    "        flatten_predict_score_df[fname] = get_construct_methods(score_df, main_detector, rsd_obj, thresholder_is_exceed_matrix_collection[fname],\n",
    "                                                                a_col)\n",
    "\n",
    "        # write out rsj matrix\n",
    "        pd.DataFrame(rank_score_diff_obj_collection_dict[fname].o_d_i).to_excel(\"/Users/kadima/experiment_any/anomaly-detection/disagreement_matrix/engineering_version/rsj_matrix_records/\"+fname+\"_odi.xlsx\")\n",
    "        pd.DataFrame(rank_score_diff_obj_collection_dict[fname].i_d_o).to_excel(\"/Users/kadima/experiment_any/anomaly-detection/disagreement_matrix/engineering_version/rsj_matrix_records/\"+fname + \"_ido.xlsx\")\n",
    "        pd.DataFrame(rank_score_diff_obj_collection_dict[fname].sum_self).to_excel(\"/Users/kadima/experiment_any/anomaly-detection/disagreement_matrix/engineering_version/rsj_matrix_records/\"+fname + \"_self_sum.xlsx\")\n",
    "        print(\"\\tDone in calculate construct\")\n",
    "\n",
    "        auc_res[fname] = flatten_predict_score_df[fname].apply(lambda x: metrics.roc_auc_score(base_results[fname].y, x), axis=0)\n",
    "        print(\"\\tDone in calculate all auc\")\n",
    "        auc_res[fname] = auc_res[fname].append(pd.Series([best_auc], index=[\"best_base_auc\"]))\n",
    "\n",
    "        # write out the auc_res\n",
    "        auc_res[fname].reset_index().to_excel('/Users/kadima/experiment_any/anomaly-detection/disagreement_matrix/engineering_version/auc_results/'+fname+\"_auc_collection.xlsx\",\n",
    "                                header=['combination', 'auc'])\n",
    "\n",
    "        print(fname, 'Done')\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    print(main_detector_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
